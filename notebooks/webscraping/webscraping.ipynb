{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b471712-a75a-492a-987e-d4037fe347ee",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fcallysto%2Flesson-plans&branch=master&subPath=notebooks/webscraping/webscraping.ipynb&depth=1\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a239c9-4509-4019-bde7-2b831efd6a13",
   "metadata": {},
   "source": [
    "## Overview: Webscraping notebook\n",
    "\n",
    "In this short lesson, we discover how to collect data from tables in a webpage and put them into a Pandas dataframe for data analysis. This is called webscraping. \n",
    "\n",
    "Outline:\n",
    "\n",
    "1. Introduction to webscraping\n",
    "2. The code libraries \"requests\" and \"beautifulsoup\"\n",
    "3. A simple example: Tables of Canadian Cities, from Wikipedia.\n",
    "4. A more complex example: Canadian parliament and creating a data frame\n",
    "5. Using APIs: Canada Open Data examples\n",
    "6. Using CSV files: baseball data example\n",
    "7. Going further: dynamic webpages, Kaggle data sournce\n",
    "\n",
    "Some useful online resources:\n",
    "- https://realpython.com/beautiful-soup-web-scraper-python/\n",
    "- https://github.com/psf/requests-html\n",
    "- https://medium.com/geekculture/web-scraping-tables-in-python-using-beautiful-soup-8bbc31c5803e\n",
    "- https://careerfoundry.com/en/blog/data-analytics/open-data-sources/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e79f94-fc01-49ca-9540-7de1a54d11a0",
   "metadata": {},
   "source": [
    "## 1. Introduction to  webscraping\n",
    "\n",
    "**Webscraping** is the process of collecting (or scraping) information from a webpage, in order to use it for further analysis. \n",
    "\n",
    "This is often useful for gathering a large amount of data in a short amount of time, whether it be a large collection of numbers and text from a big table in a single webpage, or to automatically gather data from many different webpages hosting useful information. \n",
    "\n",
    "While much of this data could be collected by hand, say by copying with a pen and paper (or more likely, copying and pasting with a keyboard command), it is much more efficient to gather the data using a computer program.\n",
    "\n",
    "This notebook shows how to use some simple tools in Python to webscrape data from interesting webpages. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a1795-8064-4dfb-b28a-6fe8e8f7d12d",
   "metadata": {},
   "source": [
    "## 2. Installing libraries\n",
    "\n",
    "We begin by installing and importing some important code libraries. First, the **requests** library, which handles the process of requesting raw data from a webpage. Next, the **beautifulsoup** library, which takes the raw data from the webpage and reformats it, or interprets it into a form that is easily saved as a data frame.\n",
    "\n",
    "Normally, we just use the **import** command to make the libraries accessible in our code. However, if the libraries are not pre-installed in our system, we need to run the following two **pip install** commands. To run them in the next cell, remove the # symbol which made those lines of code inactive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25777634-690b-433f-9b9d-bfc99facab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install requests\n",
    "#pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f6baa1-2c2f-4964-a834-c231199424e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604ecce-9bac-42d1-a1de-ebeb9150f438",
   "metadata": {},
   "source": [
    "## 3. A simple example: Scaping a list of Canadian cities\n",
    "\n",
    "We start with a URL pointing to Wikipedia, for its article on Canadian cities. A GET request will grab the data from the webpage for us. We then use the beautifulsoup library to convert the html code from the webpage into something we can use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299e9a79-7b5e-48cf-98e4-e73d497bd778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the Wikipedia webpage\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_cities_in_Canada'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Pase the webpage from the response, and save as the data item \"soup\"\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0b9ac-9e33-4ca5-9e43-a20990b37be7",
   "metadata": {},
   "source": [
    "### Looking at the response\n",
    "\n",
    "We can now look at the informuation, using the \"prettify\" instruction on \"soup.\" Since the result is quite long, let's just print out the first 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f69df-1f64-42ce-8fc5-117122075729",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()[:1000]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064b3f8a-4c06-48d4-9418-f9da9d361a03",
   "metadata": {},
   "source": [
    "### Grabbing a table\n",
    "\n",
    "The webpage contains a number of tables. We use beautifulsoup to identify each one, and save them all in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bef94b-8179-4a75-af57-27c3b9a092b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the tables and count how many there are\n",
    "tables = soup.find_all('table')\n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3828daf-42f2-4223-8ee2-2181dbd3fb1c",
   "metadata": {},
   "source": [
    "### Viewing a table\n",
    "\n",
    "Let's take a look at the first table. The command \"tables[0]\" will print out the information in the first table, which we see is a list of capital cities. The table is made up of two columns, like this:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/capitals.jpg\" alt=\"A table showing the capital cities\" width=\"400\"/><br>\n",
    "The first table on the Wiki page, showing two columns: regions and capital cities.\n",
    "</div>\n",
    "\n",
    "Here is the command to display the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead64db9-4ca9-4c96-9626-d953004d451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28ca5d-c12b-4898-b4d3-a998b09e731b",
   "metadata": {},
   "source": [
    "### Understanding the table data\n",
    "\n",
    "We see the table consists of a number of items with tags like \\<tr> or \\<td> and some text data as well.\n",
    "\n",
    "The \\<tr> indicates a row in the table, while the \\<td>  tags a piece of data in in that row.\n",
    "\n",
    "We can write a simple loop to put this information into an array we call \"contents\" and then convert it to a dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c47c81-5313-48c5-bba7-f827438b29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "\n",
    "for row in tables[0].tbody.find_all('tr'):    \n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    if(columns != []):\n",
    "        cell = {}\n",
    "        cell['Region'] = columns[0].text.strip()\n",
    "        cell['City']  = columns[1].text.strip()\n",
    "        contents.append(cell)\n",
    "\n",
    "contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a0178-28e1-4bff-95c1-37ab089cfba1",
   "metadata": {},
   "source": [
    "### Viewing the data frame\n",
    "\n",
    "Here we create the data frame and view it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ee21a-346a-44ad-9759-74a1af052961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(contents)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf84b250-4d8e-42ec-9a84-d240a802aaf9",
   "metadata": {},
   "source": [
    "### Viewing another table about Canadian cities\n",
    "\n",
    "Let's take a look at the next table from the same Wikipedia webpage. The command \"tables[1]\" will print out the information in the first table, which is information about the cities in Alberta. It looks like this:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/alberta.jpg\" alt=\"A table showing the capital cities\" width=\"800\"/><br>\n",
    "The second table on the Wiki page, showing cities in Alberta.\n",
    "</div>\n",
    "\n",
    "Run the following cell to see the raw data. It is a bit long, though, so we just print the first 2000 characters to see the basic format of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65662812-90c6-49b7-b028-6927047db8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(tables[1])[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ff6fc-a35c-4f33-b282-4fc7f2f505d7",
   "metadata": {},
   "source": [
    "### Creating another data frame\n",
    "\n",
    "While we could use all the data from the table, we will just take the name, region, and population information. From the table shown above, we see this corresponds to columns 0, 1 and 4. \n",
    "\n",
    "The code to create the data frame is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52de7b8-3c9d-4011-8ac7-742d1607cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "\n",
    "for row in tables[1].tbody.find_all('tr'):    \n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    if(columns != []):\n",
    "        cell = {}\n",
    "        cell['Name'] = columns[0].text.strip()\n",
    "        cell['Region']  = columns[1].text.strip()\n",
    "        cell['Population (2021)']  = columns[4].text.strip()\n",
    "        contents.append(cell)\n",
    "\n",
    "df1 = pd.DataFrame(contents)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff348b8-b990-478a-a101-cb78e42fd70a",
   "metadata": {},
   "source": [
    "### Describing the data\n",
    "\n",
    "We can get a quick summary of the data by using the \"describe\" function on the data frame, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de9763-8505-4038-83d1-2c40f926750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab987a0-b9c7-4838-b733-9ac6a4bbbb95",
   "metadata": {},
   "source": [
    "### From strings to numbers\n",
    "\n",
    "You may have noticed that the population data is being treated as string, not actual numbers in the data frame. \n",
    "\n",
    "We can use the following lines of code to convert these strings to numbers. First, remove the commas from the text, then convert the text to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8819895-cf13-43a5-90f9-63b6045421ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Population (2021)'] = df1['Population (2021)'].apply(lambda x: x.replace(',',''))\n",
    "df1['Population (2021)'] = df1['Population (2021)'].apply(int)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea57d73-7669-4e8a-9470-663597578a05",
   "metadata": {},
   "source": [
    "### Describing the numerical data\n",
    "\n",
    "Now that the popolulation column is representing actual numbers, the \"describe\" function will give us some basic statistics about these numbers. Such as the minimum, maximum, mean, standard deviation, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82279f00-00cf-470e-b568-b512e514ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790433b-4642-4eb0-bef3-0d4578df0539",
   "metadata": {},
   "source": [
    "## 4. A more complex example: Canadian parliament webpage\n",
    "\n",
    "For our second demonstration, let's go to a webpage that shows a list of Members of Parliament who are speaking one day in the Canadian House of Commons. There is a table on this webpage, https://openparliament.ca/debates/, and we will examine one particular date. Say, Feburary 29, 2024. The webpage looks like this:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/parliament.jpg\" alt=\"The speakers in parliament webpage\" width=\"400\"/><br>\n",
    "The first table on the Wiki page, showing two columns: regions and capital cities.\n",
    "</div>\n",
    "\n",
    "Here is the code to grab the data, using the requests library, and beautifulsoup to parse the data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b858c1-b3fe-4bdb-b53f-b583507df5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateOfDebate = ('2024/02/29/')\n",
    "\n",
    "page = requests.get('https://openparliament.ca/debates/' + dateOfDebate + '?singlepage=1').text  #?singlepage=1' gets all of the speakers\n",
    "data = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef498a08-2dc8-427f-a5b1-ba040752a891",
   "metadata": {},
   "source": [
    "While we could print out this data and examine its structure, using the print(data) command, instead let's focus on a section that is informative such as this one:\n",
    "\n",
    "```\n",
    "<div class=\"row statement_browser statement\" data-floor=\"\" data-hocid=\"12614875\" data-url=\"/debates/2024/2/29/anita-anand-1/\" id=\"sanita-anand-1\">\n",
    "<div class=\"l-ctx-col\">\n",
    "<noscript><p><a href=\"/debates/2024/2/29/anita-anand-1/only/\">Permalink</a></p></noscript>\n",
    "<p><strong class=\"statement_topic\">Main Estimates, 2024-25</strong><span class=\"br\"></span>Routine Proceedings</p>\n",
    "<p>10:10 a.m.\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\n",
    "\t\t\t\t<p>Oakville\n",
    "\t\t\t\t<span class=\"br\"></span>Ontario</p><p class=\"partytag\"><span class=\"tag partytag_liberal\">Liberal</span>\n",
    "</p></p></div>\n",
    "<div class=\"text-col\">\n",
    "<a href=\"/politicians/anita-anand/\">\n",
    "<img class=\"headshot_thumb\" src=\"/media/CACHE/images/polpics/anita-anand/76708c03c398389aa08f038af639a182.jpg\"/>\n",
    "</a>\n",
    "<p class=\"speaking\">\n",
    "<a href=\"/politicians/anita-anand/\">\n",
    "<span class=\"pol_name\">Anita Anand</span>\n",
    "</a> <span class=\"partytag tag partytag_liberal\">Liberal</span><span class=\"pol_affil\">President of the Treasury Board</span>\n",
    "</p>\n",
    "```\n",
    "\n",
    "Here we see the identifier **class=\"row statement_browser statement\"** which starts a new section with a new person speaking. \n",
    "\n",
    "The tag **class=\"pol_name\"** shows the politician's name. The tag **class=\"partytag tag partytag_liberal\"** identifies the party and the tag **class=\"pol_affil\"** shows their affilication. \n",
    "\n",
    "We use these tags to build up a dictionary of unique names of people in the debate, track their party and affliation, and also count how many times they speak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316a647-9b2a-49d8-a37d-bc17c5c0a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "debateDict = {'Name': [],\n",
    "              'Party' : [],\n",
    "              'Affiliation' : [],\n",
    "              'Count' : [],\n",
    "             }\n",
    "for item in data.findAll(\"div\", class_=\"row statement_browser statement\"):\n",
    "    try:  # getting the name of the speaker\n",
    "        name = item.find('span', class_='pol_name').text\n",
    "        name = str(name)\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    try:  # if they have spoken already, we do not find their party or affiliation\n",
    "        index = debateDict['Name'].index(name)\n",
    "        indexFound = True\n",
    "    except ValueError:\n",
    "        indexFound = False\n",
    "        try:  # finding the affiliation\n",
    "            affiliation = item.find('span', class_=\"pol_affil\").text\n",
    "            affiliation = str(affiliation).replace(\"\\n\",\"\")\n",
    "            affiliation = affiliation.replace(\"\t\t\t\t\t\t\", \"\")\n",
    "        except AttributeError:\n",
    "            affiliation = 'N/A'\n",
    "        try:  # For speakers without party tags\n",
    "            party = item.find('p', class_='partytag').text\n",
    "            party = str(party).replace(\"\\n\",\"\")\n",
    "        except AttributeError:\n",
    "            party = 'N/A'\n",
    "    if indexFound:\n",
    "        debateDict[\"Count\"][index] = debateDict[\"Count\"][index] + 1\n",
    "    else:\n",
    "        debateDict['Name'].append(name)\n",
    "        debateDict['Party'].append(party)\n",
    "        debateDict['Affiliation'].append(affiliation)\n",
    "        debateDict['Count'].append(1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff30e62-e591-40d4-82a1-caf4a6bb8b0c",
   "metadata": {},
   "source": [
    "### Viewing the data\n",
    "\n",
    "From this dictionary, we create a dataframe that shows all the collected information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e25c1c-de09-4059-ba65-f6b41de947b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame.from_dict(debateDict)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4253c95a-2b00-454d-a7d9-9b2cc8926000",
   "metadata": {},
   "source": [
    "We are now ready to analyse this data in the dataframe. But let's save that for another day. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c6cf0c-418a-4000-8e13-4fe3fbd8ccf4",
   "metadata": {},
   "source": [
    "## 5. Using  APIs: Canada Open Data\n",
    "\n",
    "An API, which stands for Application Programming Interface, is  a bridge allowing different software applications to communicate and interact with each other.\n",
    "\n",
    "Imagine you're at a restaurant. The menu acts as an API because it provides an simplfied way for you to interact with the kitchen. Instead of going into the kitchen directly and asking the chef how to cook your dish, you simply order off the menu. The kitchen staff then uses the instructions provided on the menu to prepare and serve your menu.\n",
    "\n",
    "This process simplifies the process of creating a data frame from information appearing on a webpage.\n",
    "\n",
    "For instance, we can use the same openparliament source from above, to grab information and directly convert it into a data fram. \n",
    "\n",
    "Let's start with a request to a specific web address, requesting information about votes on bills in parliament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bdf67c-0518-40d4-8ef5-401de48ee5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://api.openparliament.ca/votes/?format=json&limit=100')\n",
    "df3 = pd.DataFrame(r.json()['objects'])\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff5b0b-e9d6-4c23-b0d2-8d510c27c948",
   "metadata": {},
   "source": [
    "### Wasn't that easy?\n",
    "\n",
    "If you would like to see more done with this example, please check out this other Callysto notebook, \n",
    "[open-parliament.ipynb](https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fcallysto%2Fcurriculum-notebooks&branch=master&urlpath=notebooks/curriculum-notebooks/SocialStudies/OpenParliament/open-parliament.ipynb&depth=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc1d49-4dfa-453d-9673-7e1ac47625b6",
   "metadata": {},
   "source": [
    "## 5a: More examples: Canada open data\n",
    "\n",
    "The Canadian government publishes reams of data that is freely available for analysis. We can browse through the available data sets at this web site: https://open.canada.ca/en\n",
    "\n",
    "For instance, here is a link to information about government contracts: https://search.open.canada.ca/contracts/\n",
    "\n",
    "Let's use this link and create a dataframe. We are using the API call that the website supplies, along with a resource ID that points to this particular data set. (We can find all this information on the website.) We limit the search to the first 10 items.\n",
    "\n",
    "Our code below prints out the status code, to indicate success (200) or not (403, 404, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b3eb9-70c7-492b-90c3-d817f6a7e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://open.canada.ca/data/en/api/action/datastore_search?limit=10&resource_id=fac950c0-00d5-4ec1-a4d3-9cbebf98a305'\n",
    "response = requests.get(url)\n",
    "print(response.status_code, \"  Note: 200 indicates a successful request.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5205b90-dbae-411a-817c-71263db6b8af",
   "metadata": {},
   "source": [
    "### Reading the JSON object\n",
    "\n",
    "The response contains a JSON object, which is a dictionary with many entries. Let's look at the result of the query, which holds the records of various contracts. \n",
    "\n",
    "We limit it to the first 2 items, to keep the screen clear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456b38d-79f2-484c-9906-14df3cfb704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['result']['records'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d9571-777b-4470-9ac5-d9793ebc9b22",
   "metadata": {},
   "source": [
    "### Next step - data frame\n",
    "We turn this JSON information into the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca65183-649a-4a88-9cbb-9f0f4ca6a12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame(response.json()['result']['records'])\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8fc7b-256b-461c-9aae-c3d37874e066",
   "metadata": {},
   "source": [
    "### Narrowing the results\n",
    "\n",
    "Lets look at just the name of the vendor and the value of the contract, using the following Pandas command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21031d-d93d-4379-9b6f-8f287b654fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[['vendor_name','contract_value']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501ead8-22dc-4bd4-afe4-18a414b40979",
   "metadata": {},
   "source": [
    "## 6. Using CSV files: Baseball data example\n",
    "\n",
    "Many webpages gives users access to their data by providing downloads of files containing all their information. A common format is the CSV file, or Comma Separated Values file, which is like a generic spreadsheet table. By downloading these files directly, we can load them into a Pandas dataframe immediately, skipping all the webscraping and parsing tricks discussed above. \n",
    "\n",
    "The research group Five Thirty Eight provides all kinds of open data in the form of CSV files, which are stored in their github repo at https://github.com/fivethirtyeight. For instance we see some interesting baseball data stored at this directory: \n",
    "https://github.com/fivethirtyeight/data/tree/master/foul-balls\n",
    "\n",
    "We have downloaded this baseball file from the repo to accompany this notebook so the following \"read_csv\" command will load in the file directly to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2caecef-01d6-48d1-9f1c-14682af2703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv(\"foul-balls.csv\")\n",
    "print(df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94b9e3d-6b81-4829-b556-f306512cae2e",
   "metadata": {},
   "source": [
    "### Describe\n",
    "\n",
    "The describe command gives a quick summary about this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4577c-0410-4945-8373-ceb91311b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f06f8-2366-4550-ac5e-cb14d15ad33b",
   "metadata": {},
   "source": [
    "We can now analyse this data, asking such questions as: which team hit the most foul balls; ro, in which zone did most foul balls end up?\n",
    "\n",
    "We'll leave this up to a later notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f00ea-5f6d-42b1-80d1-a7b781c5e5f1",
   "metadata": {},
   "source": [
    "## 7. Going further\n",
    "\n",
    "We have demonstrated the basics of scraping data from static web pages using the requests library and beautiful soup. We have also seen how to gather data through APIs and by downloading CSV files.\n",
    "\n",
    "### Dynamic webpages \n",
    "\n",
    "Some webpages are not static, but instead are built \"on the fly\" with code that runs when the user views the web pages. These **dynamic** webpages load information onto the webpage using Javascipt code, as a live rendering of the information. To access these types of webpages, we need more powerful tools such as Selenium or Pyppeteer. \n",
    "\n",
    "These tools launch a \"headless\" browser in the Python enviroment where the Javascript can render the webpage content locally for us, and then we can use requests and beautlifulsoup to look at the data. You can read more about the Selenium process here:\n",
    "\n",
    "https://www.selenium.dev/\n",
    "\n",
    "and also a tutorial on the setup here:\n",
    "\n",
    "https://www.zenrows.com/blog/dynamic-web-pages-scraping-python\n",
    "\n",
    "Note that Selenium no longer requires the installation of a webdriver, which had been a obstacle to using the tool within Jupyter. Currently, a simple \"pip install selenium\" will give you access to the tools you need to run a headless browser and webscrape from dynamic webpages.\n",
    "\n",
    "### Kaggle and other data sources\n",
    "\n",
    "Some popular data sources such as Kaggle (https://www.kaggle.com/datasets) provide access to a wide variety of information available for downloads. They often sponsor hackathons where people are encouraged to analyse a suite of data in a competition.\n",
    "\n",
    "To access this data, however, you need to create a \"token\" to identify yourself and gain permission to download this data. Typically, this process is free but it does require you to provide some personal information to get access. Be sure to read the privacy rules before providing any such personal information all line.\n",
    "\n",
    "\n",
    "### Good luck, and enjoy!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e7df35-09a3-496c-97eb-32b297affdc4",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb5fe4-497d-473c-b3a0-32223cae4498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
